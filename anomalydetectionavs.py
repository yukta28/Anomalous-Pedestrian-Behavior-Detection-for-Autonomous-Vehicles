# -*- coding: utf-8 -*-
"""AnomalyDetectionAVs.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Arq08vrF_6RBancSUpDbN1KK0wxyDup6
"""

import os

video_path = "/content/"  # Colabâ€™s default upload location

# List all uploaded videos
video_files = [f for f in os.listdir(video_path) if f.endswith('.mp4')]
print(f"âœ… Found {len(video_files)} videos:", video_files)

from IPython.display import display, Video

# Choose the first uploaded video
sample_video = os.path.join(video_path, video_files[0])

# Display the video in Colab
display(Video(sample_video, embed=True))

import cv2

frame_save_path = "/content/JAAD_frames/"  # Folder for frames
os.makedirs(frame_save_path, exist_ok=True)

cap = cv2.VideoCapture(sample_video)
frame_count = 0

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    if frame_count % 10 == 0:  # Save every 10th frame
        frame_filename = f"{frame_save_path}frame_{frame_count}.jpg"
        cv2.imwrite(frame_filename, frame)

    frame_count += 1

cap.release()
print(f"âœ… Extracted frames saved in: {frame_save_path}")

!pip install ultralytics

from ultralytics import YOLO
import glob

# Load YOLOv8 pre-trained model
model = YOLO("yolov8n.pt")

# Get list of extracted frames
frames = glob.glob(frame_save_path + "*.jpg")

# Run pedestrian detection on the first frame
sample_frame = frames[0]
# Run YOLOv8 on the sample frame
results = model(sample_frame)  # Returns a list

# Access the first result and display it
results[0].show()

# Display first few rows of pedestrian tracking data
print("âœ… Pedestrian Detection Results:")
print(df.head())  # Show first few rows

# Optionally, save the results to a CSV file for further analysis
df.to_csv("/content/pedestrian_tracking_results.csv", index=False)
print("âœ… Pedestrian tracking data saved as pedestrian_tracking_results.csv")

from IPython.display import display

display(df)

import cv2

# Process each frame and save the output
for frame in frames:
    results = model(frame)[0]  # Run YOLO

    # Load image
    img = cv2.imread(frame)

    for det in results.boxes.data:
        x, y, w, h, conf, cls = det.tolist()

        if int(cls) == 0:  # Draw only pedestrians
            x, y, w, h = int(x), int(y), int(w), int(h)
            cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 2)  # Blue box
            cv2.putText(img, f"Person {conf:.2f}", (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)

    # Save output frame
    output_path = frame.replace("JAAD_frames", "JAAD_detected_frames")
    cv2.imwrite(output_path, img)

print("âœ… Detection results saved with bounding boxes!")

# Ensure data is sorted by frame order and pedestrian ID
df.sort_values(["frame", "x"], inplace=True)

# Compute velocity (change in x, y over frames)
df["vx"] = df.groupby("frame")["x"].diff()  # Change in X per frame
df["vy"] = df.groupby("frame")["y"].diff()  # Change in Y per frame

# Compute acceleration (change in velocity per frame)
df["ax"] = df.groupby("frame")["vx"].diff()
df["ay"] = df.groupby("frame")["vy"].diff()

# Display first few rows of pedestrian motion data
print("âœ… Pedestrian Motion Data:")
print(df.head())  # Show first few rows

# Optionally, save results to a CSV file
df.to_csv("/content/pedestrian_motion_data.csv", index=False)
print("âœ… Pedestrian motion data saved as pedestrian_motion_data.csv")

from google.colab import files

files.download("/content/pedestrian_motion_data.csv")

# Ensure data is sorted by frame order
df.sort_values(["frame", "x"], inplace=True)

# Compute velocity (change in x, y per frame)
df["vx"] = df.groupby("frame")["x"].diff()  # Change in X per frame
df["vy"] = df.groupby("frame")["y"].diff()  # Change in Y per frame

# Compute acceleration (change in velocity per frame)
df["ax"] = df.groupby("frame")["vx"].diff()
df["ay"] = df.groupby("frame")["vy"].diff()

from IPython.display import display

# Display first few rows of the pedestrian motion data
print("âœ… Pedestrian Motion Data:")
display(df.head())

# Save to CSV file for later use
df.to_csv("/content/pedestrian_motion_data.csv", index=False)
print("âœ… Pedestrian motion data saved as pedestrian_motion_data.csv")

import numpy as np

# Select relevant motion features
features = ["vx", "vy", "ax", "ay"]

# Drop NaN values created by differencing
df.dropna(inplace=True)

# Convert to numpy array
motion_data = df[features].values  # Extract only motion features

# Define sequence length
sequence_length = 10  # Number of frames per sequence

# Create sequences using a sliding window
sequences = []
for i in range(len(motion_data) - sequence_length):
    sequences.append(motion_data[i : i + sequence_length])  # Take 10-frame sequences

# Convert list to NumPy array
sequences = np.array(sequences)

print(f"âœ… Created {sequences.shape[0]} sequences of shape {sequences.shape}")

import torch
import torch.nn as nn
import torch.optim as optim

class LSTMAutoencoder(nn.Module):
    def __init__(self, input_dim, hidden_dim=32, latent_dim=16):
        super(LSTMAutoencoder, self).__init__()

        # Encoder
        self.lstm1 = nn.LSTM(input_dim, hidden_dim, batch_first=True)
        self.lstm2 = nn.LSTM(hidden_dim, latent_dim, batch_first=True)

        # Decoder
        self.lstm3 = nn.LSTM(latent_dim, hidden_dim, batch_first=True)
        self.lstm4 = nn.LSTM(hidden_dim, input_dim, batch_first=True)

    def forward(self, x):
        _, (h, _) = self.lstm1(x)
        _, (h, _) = self.lstm2(h)
        _, (h, _) = self.lstm3(h)
        out, _ = self.lstm4(h)
        return out

# Initialize LSTM model
input_dim = len(features)  # 4 features: vx, vy, ax, ay
model = LSTMAutoencoder(input_dim)

# Loss and optimizer
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

from torch.utils.data import DataLoader, TensorDataset

# Convert data to tensors
X = torch.tensor(sequences, dtype=torch.float32)

# Create DataLoader
train_loader = DataLoader(TensorDataset(X), batch_size=8, shuffle=True)

# Train LSTM Autoencoder
epochs = 20
for epoch in range(epochs):
    total_loss = 0
    for batch in train_loader:
        x_batch = batch[0]
        optimizer.zero_grad()
        reconstructed = model(x_batch)
        loss = criterion(reconstructed, x_batch)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    print(f"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}")

# Define anomaly detection function
def detect_anomalies(model, test_loader, threshold=0.02):
    model.eval()
    anomalies = []

    with torch.no_grad():
        for batch in test_loader:
            x_batch = batch[0]
            reconstructed = model(x_batch)
            loss = criterion(reconstructed, x_batch).item()
            if loss > threshold:  # If reconstruction error is high, it's an anomaly
                anomalies.append(loss)

    return anomalies

# Detect anomalies in test data
test_loader = DataLoader(TensorDataset(X), batch_size=8, shuffle=False)
anomalies = detect_anomalies(model, test_loader)

import matplotlib.pyplot as plt

plt.figure(figsize=(10, 5))
plt.plot(range(len(anomalies)), anomalies, marker='o', linestyle='-', color='r')
plt.axhline(y=0.02, color='g', linestyle='--', label="Threshold")
plt.title("Anomalous Pedestrian Movement Detection")
plt.xlabel("Test Sequence")
plt.ylabel("Reconstruction Error")
plt.legend()
plt.show()

import numpy as np

# Define anomaly threshold (modify based on your observation)
threshold = 20000  # Adjust based on your dataset

# Identify indices where reconstruction error > threshold
anomalous_indices = np.where(np.array(anomalies) > threshold)[0]

print("ðŸš¨ Detected Anomalous Sequences at:", anomalous_indices)

import cv2
from IPython.display import display, Image

# Show first detected anomaly
if len(anomalous_indices) > 0:
    anomaly_frame_index = anomalous_indices[0]  # First anomaly detected
    anomaly_frame_path = frames[anomaly_frame_index]

    # Display the frame
    display(Image(filename=anomaly_frame_path))
else:
    print("âœ… No anomalies detected above the threshold!")

# Use mean + 2 standard deviations as dynamic threshold
threshold = np.mean(anomalies) + 2 * np.std(anomalies)
print("ðŸ“Š New Dynamic Anomaly Threshold:", threshold)

import pandas as pd

# Save anomalies in a CSV
anomaly_df = pd.DataFrame({
    "sequence_id": anomalous_indices,
    "reconstruction_error": np.array(anomalies)[anomalous_indices]
})

anomaly_df.to_csv("/content/detected_anomalies.csv", index=False)
print("âœ… Anomaly detection results saved as detected_anomalies.csv")

import matplotlib.pyplot as plt

# Plot pedestrian trajectory
plt.figure(figsize=(10, 5))

for pedestrian_id in df["frame"].unique():
    pedestrian_df = df[df["frame"] == pedestrian_id]
    plt.plot(pedestrian_df["x"], pedestrian_df["y"], marker="o", linestyle="-", alpha=0.5)

plt.xlabel("X Position")
plt.ylabel("Y Position")
plt.title("Pedestrian Trajectories Over Time")
plt.show()

plt.figure(figsize=(8, 6))
plt.scatter(df["vx"], df["vy"], c=df["ax"], cmap="coolwarm", alpha=0.7)
plt.xlabel("Velocity X")
plt.ylabel("Velocity Y")
plt.title("Velocity vs. Acceleration of Pedestrians")
plt.colorbar(label="Acceleration")
plt.show()

plt.figure(figsize=(10, 5))
plt.hist(anomalous_indices, bins=10, color="red", alpha=0.7)
plt.xlabel("Frame Number")
plt.ylabel("Anomalous Detections")
plt.title("Anomalies Per Frame")
plt.show()

import cv2
from IPython.display import display, Image

# Load a sample frame with an anomaly
if len(anomalous_indices) > 0:
    anomaly_frame_index = anomalous_indices[0]
    anomaly_frame_path = frames[anomaly_frame_index]

    # Load image and draw annotation
    img = cv2.imread(anomaly_frame_path)
    cv2.putText(img, "Anomaly Detected!", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)

    # Save and display the frame
    cv2.imwrite("/content/anomaly_frame.jpg", img)
    display(Image(filename="/content/anomaly_frame.jpg"))

import seaborn as sns
import matplotlib.pyplot as plt

# Plot heatmap of pedestrian positions
plt.figure(figsize=(10, 6))
sns.kdeplot(x=df["x"], y=df["y"], cmap="coolwarm", fill=True)

plt.xlabel("X Position")
plt.ylabel("Y Position")
plt.title("Heatmap of Pedestrian Movement")
plt.show()

!pip install opencv-python

import cv2
import os

# Define paths
frame_folder = "/content/JAAD_frames/"  # Folder with extracted frames
output_video_path = "/content/anomaly_detection_with_boxes.mp4"

# Load a sample frame to get video dimensions
sample_frame = cv2.imread(os.path.join(frame_folder, os.listdir(frame_folder)[0]))
height, width, _ = sample_frame.shape

# Define video writer
fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for MP4 format
video_writer = cv2.VideoWriter(output_video_path, fourcc, 10, (width, height))  # FPS = 10

# Process each frame
for idx, frame_name in enumerate(sorted(os.listdir(frame_folder))):
    frame_path = os.path.join(frame_folder, frame_name)
    frame = cv2.imread(frame_path)

    # Get bounding box data for this frame
    detections = df[df["frame"] == frame_name]  # Filter detections for this frame

    for _, row in detections.iterrows():
        x, y, w, h = int(row["x"]), int(row["y"]), int(row["width"]), int(row["height"])
        color = (0, 255, 0)  # Green for normal pedestrians

        # If this frame is in anomaly list, highlight it
        if idx in anomalous_indices:
            color = (0, 0, 255)  # Red for anomalies
            cv2.putText(frame, "Anomaly Detected!", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)

        # Draw bounding box
        cv2.rectangle(frame, (x, y), (x + w, y + h), color, 3)

    # Write frame to video
    video_writer.write(frame)

# Release video writer
video_writer.release()

print("âœ… Video saved at:", output_video_path)

from IPython.display import display, Video

display(Video(output_video_path, embed=True))

from google.colab import files

files.download(output_video_path)

import cv2
import numpy as np
import pandas as pd

# Define paths
input_video_path = "/content/video_0004.mp4"  # Path to original video
output_video_path = "/content/video_0004_with_anomalies.mp4"  # Output video path

# Load video
cap = cv2.VideoCapture(input_video_path)

# Get video properties
frame_width = int(cap.get(3))
frame_height = int(cap.get(4))
fps = int(cap.get(cv2.CAP_PROP_FPS))

# Define Video Writer
fourcc = cv2.VideoWriter_fourcc(*'mp4v')
out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))

# Data storage for tracking pedestrians
tracking_data = []
frame_count = 0

from ultralytics import YOLO

# Load the pre-trained YOLOv8 model (COCO dataset includes 'person' class)
model = YOLO("yolov8m.pt")  # Small and efficient model

while cap.isOpened():
    ret, frame = cap.read()

    if not ret:
        break

    frame_count += 1
    detections = df[df["frame"] == frame_count]  # Get detections for this frame

    for _, row in detections.iterrows():
        x, y, w, h = int(row["x"]), int(row["y"]), int(row["width"]), int(row["height"])
        color = (0, 255, 0)  # Green for normal pedestrians

        if row["anomaly"] == 1:  # Anomalous pedestrian
            color = (0, 0, 255)  # Red bounding box for anomalies
            cv2.putText(frame, "ðŸš¨ Anomaly Detected!", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)

        # Draw bounding box
        cv2.rectangle(frame, (x, y), (x + w, y + h), color, 3)

    # Show the processed frame for debugging
    from google.colab.patches import cv2_imshow

    cv2_imshow(frame)  # Display frame in Colab

    cv2.waitKey(1)  # Display frame for debugging (press any key to continue)

    # Write processed frame to video
    out.write(frame)

# Release resources
cap.release()
out.release()
cv2.destroyAllWindows()

# Reload video for visualization
cap = cv2.VideoCapture(input_video_path)
out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))

frame_count = 0
while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    frame_count += 1
    detections = df[df["frame"] == frame_count]  # Get detections for this frame

    for det in results.boxes.data:
      x, y, w, h, conf, cls = det.tolist()  # Extract bounding box data

    if int(cls) == 0:  # Class 0 = "person"
        # Convert from relative to absolute pixel values
        x = int(x * frame_width)
        y = int(y * frame_height)
        w = int(w * frame_width)
        h = int(h * frame_height)

        # Draw bounding box
        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 3)
        cv2.putText(frame, f"Person {conf:.2f}", (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)

        # Draw bounding box
        cv2.rectangle(frame, (x, y), (x + w, y + h), color, 3)

    # Write annotated frame to video
    out.write(frame)

# Release resources
cap.release()
out.release()

print("âœ… Final video with detected anomalies saved:", output_video_path)

from google.colab import files

files.download(output_video_path)

from ultralytics import YOLO
import glob

# Load YOLOv8 model
model = YOLO("yolov8m.pt")

# Get list of extracted frames
frames = glob.glob(frame_save_path + "*.jpg")

# Loop through all frames and run YOLOv8
for frame in frames:
    results = model(frame)  # Run YOLOv8 detection
    results[0].show()  # Display results for each frame

import cv2
import os
from ultralytics import YOLO

# Load YOLOv8 model
model = YOLO("yolov8m.pt")

# Define paths
input_video_path = "/content/video_0004.mp4"  # Path to your video
output_folder = "/content/processed_frames/"  # Folder to save processed frames

# Create output folder if not exists
os.makedirs(output_folder, exist_ok=True)

# Open video file
cap = cv2.VideoCapture(input_video_path)
frame_count = 0

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break  # Stop if video ends

    # Run YOLOv8 on the frame
    results = model(frame)

    # Draw bounding boxes on the frame
    annotated_frame = results[0].plot()

    # Save processed frame
    frame_path = os.path.join(output_folder, f"frame_{frame_count:04d}.jpg")
    cv2.imwrite(frame_path, annotated_frame)

    frame_count += 1

cap.release()
print(f"âœ… Processed {frame_count} frames and saved to {output_folder}")

import cv2
import os
from ultralytics import YOLO

# Load YOLOv8 model
model = YOLO("yolov8m.pt")

# Define paths
input_video_path = "/content/video_0004.mp4"  # Path to your video
output_folder = "/content/processed_frames/"  # Folder to save processed frames

# Create output folder if not exists
os.makedirs(output_folder, exist_ok=True)

# Open video file
cap = cv2.VideoCapture(input_video_path)
frame_count = 0

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break  # Stop if video ends

    # Run YOLOv8 on the frame
    results = model(frame)

    # Draw bounding boxes on the frame
    annotated_frame = results[0].plot()

    # Save processed frame
    frame_path = os.path.join(output_folder, f"frame_{frame_count:04d}.jpg")
    cv2.imwrite(frame_path, annotated_frame)

    frame_count += 1

cap.release()
print(f"âœ… Processed {frame_count} frames and saved to {output_folder}")

import glob

# Define output video path
output_video_path = "/content/output_video.mp4"

# Get all processed frames
frame_files = sorted(glob.glob(output_folder + "*.jpg"))  # Sort frames by name

# Load first frame to get dimensions
frame_sample = cv2.imread(frame_files[0])
height, width, layers = frame_sample.shape

# Define codec and video writer
fourcc = cv2.VideoWriter_fourcc(*'mp4v')
fps = 30  # Adjust based on original video
out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))

# Write each frame into the video
for frame_file in frame_files:
    frame = cv2.imread(frame_file)
    out.write(frame)

out.release()
print("âœ… Video reassembled and saved at:", output_video_path)

from google.colab import files

files.download(output_video_path)